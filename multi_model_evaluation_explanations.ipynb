{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Scikit-learn models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filename):\n",
    "    \"\"\"\n",
    "    Read the data file into a pandas dataframe, preprocess the data, and define feature and target variables.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): Path to the data file.\n",
    "\n",
    "    Globals:\n",
    "    - X (DataFrame): Features dataframe.\n",
    "    - Y (Series): Target variable series.\n",
    "\n",
    "    \"\"\"\n",
    "    global X, Y  # Declare the variables as global\n",
    "    \n",
    "    df_tidy = pd.read_excel(filename)\n",
    "    df_tidy = df_tidy.dropna()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['STAL', 'STAR', 'STAZ', 'STCA', 'STCO', 'STCT',\n",
    "       'STDE', 'STFL', 'STGA', 'STIA', 'STID', 'STIL', 'STIN', 'STKS', 'STKY',\n",
    "       'STLA', 'STMA', 'STMD', 'STME', 'STMI', 'STMN', 'STMO', 'STMS', 'STMT',\n",
    "       'STNC', 'STND', 'STNE', 'STNH', 'STNJ', 'STNM', 'STNV', 'STNY', 'STOH',\n",
    "       'STOK', 'STOR', 'STPA', 'STRI', 'STSC', 'STSD', 'STTN', 'STTX', 'STUT',\n",
    "       'STVA', 'STVT', 'STWA', 'STWI', 'STWV', 'STWY']\n",
    "    df_reduced = df_tidy.drop(columns_to_drop, axis=1)\n",
    "    df_filtered = df_reduced.drop([\"ST\", \"Town\", \"county\", \"Bank_Name\", \"State_Town\", \"State_County\"], axis=1)\n",
    "\n",
    "    # Reorder columns to move 'unique' to the end\n",
    "    df_filtered['unique'] = df_filtered.pop('unique')\n",
    "\n",
    "    # Define feature and target variables\n",
    "    features = ['lntotalassets', 'LoanAssets', 'CashDeposits_w',\n",
    "       'EquityAssets', 'lnbankage', 'bank_National', 'bank_StateMem',\n",
    "       'bankwithbranches', 'CRC', 'RC', 'CH', 'lntotpop', 'fracurbpop',\n",
    "       'fracilliteratepop', 'fracnonwhite', 'lntotal_banks_county',\n",
    "       'farmspercap', 'mfgestabpercap100', 'lnretailsales1929',\n",
    "       'pfarmswithmortgagedebt', 'fracpresdemvote1932', 'fraccongdemvote1930',\n",
    "       'n_1_bankt', 'ln_rmdr_avg_assetsperbk_town',\n",
    "       'rmdr_avg_loanassets_by_town', 'rmdr_avg_cashdeposits_by_town_w',\n",
    "       'rmdr_avg_equityassets_by_town', 'rmr_fracnat_town',\n",
    "       'rmr_fracstatemem_town','unique']\n",
    "    \n",
    "    X = df_filtered[features]\n",
    "    Y = df_filtered['ReopenedByMarch29_UR']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method that works by constructing multiple decision trees during training. The predictions of individual trees are then combined (either by averaging or by majority voting) to produce a single output. It is known for its flexibility, ability to model complex interactions, and mitigation of overfitting through the introduction of randomness in the model-building process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters_rf():\n",
    "    \"\"\"\n",
    "    Optimize the hyperparameters of a RandomForestClassifier using GridSearchCV.\n",
    "\n",
    "    The function will perform a grid search over specified parameter values \n",
    "    for a random forest classifier. The dataset used for this is assumed to be \n",
    "    global and named as `X` for the features and `Y` for the target variable.\n",
    "    The function drops the 'unique' column from the feature set before optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    None\n",
    "    \n",
    "    Returns:\n",
    "    dict: Best parameters found during the grid search.\n",
    "    \n",
    "    \"\"\"\n",
    "    global X, Y  # Declare the variables as global\n",
    "\n",
    "    \n",
    "    X_opt = X.drop(['unique'], axis=1)\n",
    "    \n",
    "    param_grid = {\n",
    "       'max_depth': [8, 10, 12, 14, 16],\n",
    "       'n_estimators': [50, 100, 150, 200],\n",
    "       'max_samples': [0.3, 0.4, 0.5]\n",
    "    }\n",
    "\n",
    "    rforest_model = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator=rforest_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_opt, Y)\n",
    "    \n",
    "    return grid_search.best_params_\n",
    "\n",
    "def run_model_rf(best_params, iter_num):\n",
    "    \"\"\"\n",
    "    Run the RandomForestClassifier model using provided optimal parameters and returns the predictions and accuracy score.\n",
    "\n",
    "    The function trains a random forest classifier using the best parameters \n",
    "    provided as input. The dataset used is assumed to be global and named as \n",
    "    `X` for the features and `Y` for the target variable. \n",
    "    The function drops the 'unique' column from the feature set before training the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - best_params (dict): The best hyperparameters obtained after optimization.\n",
    "    - iter_num (int): An identifier or iteration number, used to name the prediction column.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list containing:\n",
    "    - [0]: DataFrame with 'unique' column and the prediction column named 'predictionRF{iter_num}'\n",
    "    - [1]: Accuracy score of the model on the test set.\n",
    "    \n",
    "    \"\"\"\n",
    "    global X, Y  # Declare the variables as global\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5)\n",
    "    X_train_np = X_train.drop(['unique'], axis=1).to_numpy()\n",
    "    X_test_np = X_test.drop(['unique'], axis=1).to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    y_test_np = y_test.to_numpy()\n",
    "\n",
    "    rforest_model = RandomForestClassifier(**best_params)\n",
    "    rforest_model.fit(X_train_np, y_train_np)\n",
    "\n",
    "    predictions = rforest_model.predict(X_test_np)\n",
    "\n",
    "    df_predictions = pd.DataFrame(predictions, columns=['predicted_outcome'])\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    X_test[f'predictionRF{iter_num}'] = df_predictions['predicted_outcome']\n",
    "    df_results = X_test[[\"unique\", f'predictionRF{iter_num}']]\n",
    "\n",
    "    return [df_results, accuracy_score(y_test_np, predictions)]\n",
    "\n",
    "def loop_model_rf(num_iterations):\n",
    "    \"\"\"\n",
    "    Run the RandomForestClassifier model for a specified number of iterations using optimized parameters.\n",
    "    \n",
    "    This function first optimizes the hyperparameters of a RandomForest classifier \n",
    "    using the `optimize_parameters` function. Then, it runs the `run_model_RF` function \n",
    "    for a given number of iterations. The predictions from each iteration are combined \n",
    "    into a single DataFrame, and the accuracies from each run are stored in a list.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_iterations (int): The number of times the model should be run.\n",
    "    \n",
    "    Globals:\n",
    "    - Combined results and accuracies are stored in the global variables `combined_results` \n",
    "      and `accuracies` respectively.\n",
    "    \n",
    "    Note:\n",
    "    This function relies on the existence and proper functioning of both the \n",
    "    `optimize_parameters` and `run_model_RF` functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_params = optimize_parameters_rf()\n",
    "    global combined_results\n",
    "    global accuracies\n",
    "    combined_results = pd.DataFrame()\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        results, accuracy = run_model_rf(best_params, i)\n",
    "        if i == 0:\n",
    "            combined_results = results\n",
    "        else:\n",
    "            combined_results = pd.merge(results, combined_results, on=\"unique\", how=\"outer\")\n",
    "        accuracies.append(accuracy)\n",
    "    return [combined_results, accuracies]\n",
    "\n",
    "def save_results_rf(num_iterations=20, output_filename='RandomForestResults.csv'):\n",
    "    \"\"\"\n",
    "    Execute the random forest model multiple times, aggregate the results, \n",
    "    and save them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - num_iterations (int): Number of iterations to run the random forest model.\n",
    "    - output_filename (str): Path for the output CSV file.\n",
    "\n",
    "    \"\"\"\n",
    "    preprocess_data(\"ConcurrentExecution/RAdatafile.xlsx\")\n",
    "    # Execute the model for the given number of iterations\n",
    "    rf_combined_results = loop_model_rf(num_iterations)\n",
    "    \n",
    "    # Extract and fill missing values in the results dataframe\n",
    "    aggregated_results = rf_combined_results[0].fillna(8)\n",
    "    \n",
    "    # Group by the 'unique' column and sum the results\n",
    "    summed_results = aggregated_results.groupby('unique').sum()\n",
    "\n",
    "    # Filter out rows where any prediction column has a value of 9 or greater\n",
    "    prediction_columns = [f'predictionRF{i}' for i in range(num_iterations)]\n",
    "    filtered_results = summed_results[summed_results[prediction_columns].lt(9).all(axis=1)]\n",
    "\n",
    "    # Save the final results to a CSV file\n",
    "    filtered_results.to_csv(output_filename)\n",
    "    return filtered_results\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Support Vector Machine (SVM)\n",
    "\n",
    "Support Vector Machines are supervised learning algorithms which aim to find the optimal hyperplane that best separates the dataset into classes. Suitable for both regression and classification tasks, SVMs are particularly well-suited for classification of complex but small- or medium-sized datasets. They work by maximizing the margin between decision boundary and the closest data points from each class, known as support vectors. The capability to use kernel functions also allows SVMs to solve non-linear problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters_svm():\n",
    "    \"\"\"\n",
    "    Optimize the hyperparameters of a Support Vector Machine (SVM) using GridSearchCV.\n",
    "\n",
    "    This function performs a grid search over specified parameter values \n",
    "    for an SVM classifier. The dataset used for this optimization is assumed \n",
    "    to be global and named as `X` for the features and `Y` for the target variable. \n",
    "    The function drops the 'unique' column from the feature set before optimization.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Globals:\n",
    "    - X (DataFrame): Features dataframe. Expected to have a column named 'unique' which will be dropped.\n",
    "    - Y (Series): Target variable series.\n",
    "\n",
    "    Returns:\n",
    "    dict: Best parameters found during the grid search.\n",
    "    \"\"\"\n",
    "    X_opt = X.drop(['unique'], axis=1)\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "\n",
    "    svm_model = SVC()\n",
    "    grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_opt, Y)\n",
    "    return grid_search.best_params_\n",
    "\n",
    "def run_model_svm(best_params, iter_num):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Support Vector Machine (SVM) model using provided optimal parameters.\n",
    "\n",
    "    This function trains an SVM classifier using the best parameters \n",
    "    provided as input. The dataset used for training and evaluation is \n",
    "    assumed to be global and named as `X` for the features and `Y` for the target variable. \n",
    "    The function drops the 'unique' column from the feature set before training the model.\n",
    "    After predictions are made on the test set, the results are combined with the \n",
    "    'unique' column for identification and named according to the iteration number.\n",
    "\n",
    "    Parameters:\n",
    "    - best_params (dict): The best hyperparameters obtained after optimization.\n",
    "    - iter_num (int): An identifier or iteration number, used to name the prediction column.\n",
    "\n",
    "    Globals:\n",
    "    - X (DataFrame): Features dataframe. Expected to have a column named 'unique' which will be dropped.\n",
    "    - Y (Series): Target variable series.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing:\n",
    "    - [0]: DataFrame with 'unique' column and the prediction column named 'predictionSVM{iter_num}'\n",
    "    - [1]: Accuracy score of the model on the test set.\n",
    "\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5)\n",
    "    X_train_np = X_train.drop(['unique'], axis=1).to_numpy()\n",
    "    X_test_np = X_test.drop(['unique'], axis=1).to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    y_test_np = y_test.to_numpy()\n",
    "\n",
    "    svm_model = SVC(**best_params)\n",
    "    svm_model.fit(X_train_np, y_train_np)\n",
    "\n",
    "    predictions = svm_model.predict(X_test_np)\n",
    "\n",
    "    df_predictions = pd.DataFrame(predictions, columns=['predicted_outcome'])\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    X_test[f'predictionSVM{iter_num}'] = df_predictions['predicted_outcome']\n",
    "    df_results = X_test[[\"unique\", f'predictionSVM{iter_num}']]\n",
    "\n",
    "    return [df_results, accuracy_score(y_test_np, predictions)]\n",
    "\n",
    "def loop_model_svm(num_iterations):\n",
    "    \"\"\"\n",
    "    Run the SVM model for a specified number of iterations using optimized parameters.\n",
    "    \n",
    "    This function first optimizes the hyperparameters of an SVM classifier \n",
    "    using the `optimize_parameters_svm` function. Then, it runs the `run_model_SVM` function \n",
    "    for a given number of iterations. The predictions from each iteration are combined \n",
    "    into a single DataFrame, and the accuracies from each run are stored in a list.\n",
    "\n",
    "    Parameters:\n",
    "    - num_iterations (int): The number of times the SVM model should be run.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing:\n",
    "    - [0]: DataFrame with 'unique' column and the prediction columns named 'predictionSVM{iter_num}' for each iteration.\n",
    "    - [1]: A list of accuracy scores of the model for each iteration.\n",
    "\n",
    "    Note:\n",
    "    This function relies on the existence and proper functioning of both the \n",
    "    `optimize_parameters_svm` and `run_model_SVM` functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_params = optimize_parameters_svm()\n",
    "    combined_results = pd.DataFrame()\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        print(i)\n",
    "        results, accuracy = run_model_SVM(best_params, i)\n",
    "        if i == 0:\n",
    "            combined_results = results\n",
    "        else:\n",
    "            combined_results = pd.merge(results, combined_results, on=\"unique\", how=\"outer\")\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return [combined_results, accuracies]\n",
    "\n",
    "def save_results_svm(num_iterations=20, output_filename='SVMResults.csv'):\n",
    "    \"\"\"\n",
    "    Execute the SVM model multiple times, aggregate the results, \n",
    "    and save them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - num_iterations (int): Number of iterations to run the SVM model.\n",
    "    - output_filename (str): Path for the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The filtered aggregated results from the SVM model runs.\n",
    "\n",
    "    \"\"\"\n",
    "    preprocess_data(\"ConcurrentExecution/RAdatafile.xlsx\")\n",
    "\n",
    "    # Execute the model for the given number of iterations\n",
    "    svm_combined_results = loop_model_svm(num_iterations)\n",
    "    \n",
    "    # Extract and fill missing values in the results dataframe\n",
    "    aggregated_results = svm_combined_results[0].fillna(8)\n",
    "    \n",
    "    # Group by the 'unique' column and sum the results\n",
    "    summed_results = aggregated_results.groupby('unique').sum()\n",
    "\n",
    "    # Filter out rows where any prediction column has a value of 9 or greater\n",
    "    prediction_columns = [f'predictionSVM{i}' for i in range(num_iterations)]\n",
    "    filtered_results = summed_results[summed_results[prediction_columns].lt(9).all(axis=1)]\n",
    "\n",
    "    # Save the final results to a CSV file\n",
    "    filtered_results.to_csv(output_filename)\n",
    "    \n",
    "    return filtered_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: K-Nearest Neighbor (KNN)\n",
    "\n",
    "The K-Nearest Neighbor (KNN) algorithm is a type of instance-based learning where the function is approximated locally and all computation is deferred until classification. It is a non-parametric, lazy learning algorithm. When queried for a classification, KNN does not use any trained model but rather performs the classification based on the majority class among the K-most similar instances from the training dataset. The 'distance' between instances defines the similarity. Commonly used distance metrics include Euclidean, Manhattan, and Minkowski distances. The choice of K and the distance metric are critical decisions in the application of this algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters_knn():\n",
    "    \"\"\"\n",
    "    Optimize the hyperparameters of a K-Nearest Neighbors (KNN) classifier using GridSearchCV.\n",
    "\n",
    "    This function performs a grid search over specified parameter values \n",
    "    for a KNN classifier. The dataset used for this optimization is assumed \n",
    "    to be global and named as `X` for the features and `Y` for the target variable. \n",
    "    The function drops the 'unique' column from the feature set before optimization.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Globals:\n",
    "    - X (DataFrame): Features dataframe. Expected to have a column named 'unique' which will be dropped.\n",
    "    - Y (Series): Target variable series.\n",
    "\n",
    "    Returns:\n",
    "    dict: Best parameters found during the grid search.\n",
    "    \"\"\"\n",
    "    X_opt = X.drop(['unique'], axis=1)\n",
    "    param_grid = {\n",
    "        'n_neighbors': list(range(1, 31)),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }\n",
    "\n",
    "    knn_model = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_opt, Y)\n",
    "    return grid_search.best_params_\n",
    "\n",
    "def run_model_knn(best_params, iter_num):\n",
    "    \"\"\"\n",
    "    Train and evaluate a K-Nearest Neighbors (KNN) classifier using provided optimal parameters.\n",
    "\n",
    "    This function trains a KNN classifier using the best parameters \n",
    "    provided as input. The dataset used for training and evaluation is \n",
    "    assumed to be global and named as `X` for the features and `Y` for the target variable. \n",
    "    The function drops the 'unique' column from the feature set before training the classifier.\n",
    "    After predictions are made on the test set, the results are combined with the \n",
    "    'unique' column for identification and named according to the iteration number.\n",
    "\n",
    "    Parameters:\n",
    "    - best_params (dict): The best hyperparameters obtained after optimization.\n",
    "    - iter_num (int): An identifier or iteration number, used to name the prediction column.\n",
    "\n",
    "    Globals:\n",
    "    - X (DataFrame): Features dataframe. Expected to have a column named 'unique' which will be dropped.\n",
    "    - Y (Series): Target variable series.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing:\n",
    "    - [0]: DataFrame with 'unique' column and the prediction column named 'predictionKNN{iter_num}'\n",
    "    - [1]: Accuracy score of the classifier on the test set.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5)\n",
    "    X_train_np = X_train.drop(['unique'], axis=1).to_numpy()\n",
    "    X_test_np = X_test.drop(['unique'], axis=1).to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    y_test_np = y_test.to_numpy()\n",
    "\n",
    "    knn_model = KNeighborsClassifier(**best_params)\n",
    "    knn_model.fit(X_train_np, y_train_np)\n",
    "\n",
    "    predictions = knn_model.predict(X_test_np)\n",
    "\n",
    "    df_predictions = pd.DataFrame(predictions, columns=['predicted_outcome'])\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    X_test[f'predictionKNN{iter_num}'] = df_predictions['predicted_outcome']\n",
    "    df_results = X_test[[\"unique\", f'predictionKNN{iter_num}']]\n",
    "\n",
    "    return [df_results, accuracy_score(y_test_np, predictions)]\n",
    "\n",
    "def loop_model_knn(num_iterations):\n",
    "    \"\"\"\n",
    "    Run the KNN classifier for a specified number of iterations using optimized parameters.\n",
    "    \n",
    "    This function first optimizes the hyperparameters of a KNN classifier \n",
    "    using the `optimize_parameters_knn` function. Then, it runs the `run_model_knn` function \n",
    "    for a given number of iterations. The predictions from each iteration are combined \n",
    "    into a single DataFrame, and the accuracies from each run are stored in a list.\n",
    "\n",
    "    Parameters:\n",
    "    - num_iterations (int): The number of times the KNN classifier should be run.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing:\n",
    "    - [0]: DataFrame with 'unique' column and the prediction columns named 'predictionKNN{iter_num}' for each iteration.\n",
    "    - [1]: A list of accuracy scores of the classifier for each iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_params = optimize_parameters_knn()\n",
    "    combined_results = pd.DataFrame()\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        print(i)\n",
    "        results, accuracy = run_model_knn(best_params, i)\n",
    "        if i == 0:\n",
    "            combined_results = results\n",
    "        else:\n",
    "            combined_results = pd.merge(results, combined_results, on=\"unique\", how=\"outer\")\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return [combined_results, accuracies]\n",
    "\n",
    "def save_results_knn(num_iterations=20, output_filename='KNNResults.csv'):\n",
    "    \"\"\"\n",
    "    Execute the KNN classifier multiple times, aggregate the results, \n",
    "    and save them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - num_iterations (int): Number of iterations to run the KNN classifier.\n",
    "    - output_filename (str): Path for the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The filtered aggregated results from the KNN classifier runs.\n",
    "    \"\"\"\n",
    "    # Note: The following line was not included in your original SVM function\n",
    "    # preprocess_data(\"ConcurrentExecution/RAdatafile.xlsx\")\n",
    "\n",
    "    # Execute the classifier for the given number of iterations\n",
    "    knn_combined_results = loop_model_knn(num_iterations)\n",
    "    \n",
    "    # Extract and fill missing values in the results dataframe\n",
    "    aggregated_results = knn_combined_results[0].fillna(8)\n",
    "    \n",
    "    # Group by the 'unique' column and sum the results\n",
    "    summed_results = aggregated_results.groupby('unique').sum()\n",
    "\n",
    "    # Filter out rows where any prediction column has a value of 9 or greater\n",
    "    prediction_columns_knn = [f'predictionKNN{i}' for i in range(num_iterations)]\n",
    "    filtered_results = summed_results[summed_results[prediction_columns_knn].lt(9).all(axis=1)]\n",
    "\n",
    "    # Save the final results to a CSV file\n",
    "    filtered_results.to_csv(output_filename)\n",
    "    \n",
    "    return filtered_results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Logistic Regression (Logit)\n",
    "\n",
    "Logistic Regression is a widely used statistical method to predict a binary outcome from a linear combination of predictor variables. The central principle of Logistic Regression is the logistic function, which is an S-shaped curve that can take any real-valued number and map it between 0 and 1. This is useful in scenarios where the response variable is categorical, typically with two categories/classes. The coefficients of the logistic regression model are estimated from the training data using the maximum likelihood estimation. Regularization techniques, such as L1 (Lasso) and L2 (Ridge), can be applied to prevent overfitting and handle multicollinearity in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters_logit():\n",
    "    \"\"\"\n",
    "    Optimize the hyperparameters of a Logistic Regression classifier using GridSearchCV.\n",
    "\n",
    "    This function performs a grid search over specified parameter values \n",
    "    for a Logistic Regression classifier. The dataset used for this optimization \n",
    "    is assumed to be global and named as `X` for the features and `Y` for the \n",
    "    target variable. The function drops the 'unique' column from the feature set \n",
    "    before optimization.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Globals:\n",
    "    - X (DataFrame): Features dataframe. Expected to have a column named 'unique' which will be dropped.\n",
    "    - Y (Series): Target variable series.\n",
    "\n",
    "    Returns:\n",
    "    dict: Best parameters found during the grid search.\n",
    "    \"\"\"\n",
    "    X_opt = X.drop(['unique'], axis=1)\n",
    "    param_grid = {\n",
    "        'C': np.logspace(-4, 4, 20),  # Regularization strength, inverse of lambda\n",
    "        'penalty': ['l1', 'l2'],  # Lasso and Ridge\n",
    "        'solver': ['liblinear']  # This solver works well with both L1 and L2 regularization\n",
    "    }\n",
    "\n",
    "    logit_model = LogisticRegression(max_iter=10000)\n",
    "    grid_search = GridSearchCV(estimator=logit_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_opt, Y)\n",
    "    return grid_search.best_params_\n",
    "\n",
    "def run_model_logit(best_params, iter_num):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Logistic Regression classifier using provided optimal parameters.\n",
    "\n",
    "    This function trains a Logistic Regression classifier using the best \n",
    "    parameters provided as input. The dataset used for training and evaluation \n",
    "    is assumed to be global and named as `X` for the features and `Y` for the \n",
    "    target variable. The function drops the 'unique' column from the feature \n",
    "    set before training the classifier. After predictions are made on the test \n",
    "    set, the results are combined with the 'unique' column for identification \n",
    "    and named according to the iteration number.\n",
    "\n",
    "    Parameters:\n",
    "    - best_params (dict): The best hyperparameters obtained after optimization.\n",
    "    - iter_num (int): An identifier or iteration number, used to name the prediction column.\n",
    "\n",
    "    Globals:\n",
    "    - X (DataFrame): Features dataframe. Expected to have a column named 'unique' which will be dropped.\n",
    "    - Y (Series): Target variable series.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing:\n",
    "    - [0]: DataFrame with 'unique' column and the prediction column named 'predictionLogit{iter_num}'\n",
    "    - [1]: Accuracy score of the classifier on the test set.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5)\n",
    "    X_train_np = X_train.drop(['unique'], axis=1).to_numpy()\n",
    "    X_test_np = X_test.drop(['unique'], axis=1).to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    y_test_np = y_test.to_numpy()\n",
    "\n",
    "    logit_model = LogisticRegression(**best_params, max_iter=10000)\n",
    "    logit_model.fit(X_train_np, y_train_np)\n",
    "\n",
    "    predictions = logit_model.predict(X_test_np)\n",
    "\n",
    "    df_predictions = pd.DataFrame(predictions, columns=['predicted_outcome'])\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    X_test[f'predictionLogit{iter_num}'] = df_predictions['predicted_outcome']\n",
    "    df_results = X_test[[\"unique\", f'predictionLogit{iter_num}']]\n",
    "\n",
    "    return [df_results, accuracy_score(y_test_np, predictions)]\n",
    "\n",
    "def loop_model_logit(num_iterations):\n",
    "    \"\"\"\n",
    "    Run the Logistic Regression classifier for a specified number of iterations using optimized parameters.\n",
    "\n",
    "    This function first optimizes the hyperparameters of a Logistic Regression classifier \n",
    "    using the `optimize_parameters_logit` function. Then, it runs the `run_model_logit` function \n",
    "    for a given number of iterations. The predictions from each iteration are combined \n",
    "    into a single DataFrame, and the accuracies from each run are stored in a list.\n",
    "\n",
    "    Parameters:\n",
    "    - num_iterations (int): The number of times the Logistic Regression classifier should be run.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing:\n",
    "    - [0]: DataFrame with 'unique' column and the prediction columns named 'predictionLogit{iter_num}' for each iteration.\n",
    "    - [1]: A list of accuracy scores of the classifier for each iteration.\n",
    "    \"\"\"\n",
    "    best_params = optimize_parameters_logit()\n",
    "    combined_results = pd.DataFrame()\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        results, accuracy = run_model_logit(best_params, i)\n",
    "        if i == 0:\n",
    "            combined_results = results\n",
    "        else:\n",
    "            combined_results = pd.merge(results, combined_results, on=\"unique\", how=\"outer\")\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return [combined_results, accuracies]\n",
    "\n",
    "def save_results_logit(num_iterations=20, output_filename='LogitResults.csv'):\n",
    "    \"\"\"\n",
    "    Execute the Logistic Regression classifier multiple times, aggregate the results, \n",
    "    and save them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - num_iterations (int): Number of iterations to run the Logistic Regression classifier.\n",
    "    - output_filename (str): Path for the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The filtered aggregated results from the Logistic Regression classifier runs.\n",
    "    \"\"\"\n",
    "    # Execute the classifier for the given number of iterations\n",
    "    logit_combined_results = loop_model_logit(num_iterations)\n",
    "    \n",
    "    # Extract and fill missing values in the results dataframe\n",
    "    aggregated_results = logit_combined_results[0].fillna(8)\n",
    "    \n",
    "    # Group by the 'unique' column and sum the results\n",
    "    summed_results = aggregated_results.groupby('unique').sum()\n",
    "\n",
    "    # Filter out rows where any prediction column has a value of 9 or greater\n",
    "    prediction_columns_logit = [f'predictionLogit{i}' for i in range(num_iterations)]\n",
    "    filtered_results = summed_results[summed_results[prediction_columns_logit].lt(9).all(axis=1)]\n",
    "\n",
    "    # Save the final results to a CSV file\n",
    "    filtered_results.to_csv(output_filename)\n",
    "    \n",
    "    return filtered_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
